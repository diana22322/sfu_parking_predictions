DESCRIPTION:
------------
This folder contains machine learning component of the project. We executed all the steps of this component in Sagemaker. We executed the following process:
- feature engineering: Combined the historical and real-time event data (stored in S3 data lake) and aggregated into a training set. This process created time-series features and corresponding future outcomes (labels). 
- Exploration & Visualization: Performed data exploration in a jupyter notebook to validate the synthetic data's realism, understand trends, and ensure the quality of the engineered features. 
- Model training: Trained two independent XGBoost models to make 2 predictions (occupancy in 15 minutes, number of departures in next 15 minutes)
- Deployment: Deployed both models as Sagemaker endpoints, which are then consumed by the scheduled Lambda Predictor Service (Lambda C)


FOLDER STRUCTURE:
----------------
sagemaker/
├── models
│   └── model_training.py
├── notebooks
│   ├── 1_data_exploration_and_visualization.ipynb
│   └── 2_model_training.ipynb
└── feature_engineering
    └── sfu_feature_engineering.py


CONTENTS:
--------

. sfu_feature_engineering.py - 
Purpose: Source code for the PySpark feature engineering job. It reads the combined raw parquet data from S3, calculates rolling occupancy features, and created two predictive labels: occupancy_plus_15m and departurs_in_next_15m
Output: the final feature training set, saved back to S3 for consumption by Sagmaker 

. 1_data_exploration_and_visualization.ipynb
Purpose: To validate the synthetic data generated by the pipeline, confirm relationships between variables, and distribution of data. 
Input: the final training feature set from S3

. 2_model_training.ipynb - 
Purpose: Production ready script used to execute the final model training and deployment. It performs data splitting, training, and deployment of two final Sagemaker endpoints

. model_training.py - This is the same model training code as in the above notebook file. 

